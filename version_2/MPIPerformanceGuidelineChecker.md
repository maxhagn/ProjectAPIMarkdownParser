<small>**üöÄ Explore my digital universe: [maximilian.hagn.network](https://maximilian.hagn.network)!</small>**

# MPI Performance Guidelines (2. BSc Thesis)

![Thumbnail](https://files.hagn.network/images/mpi-performance-guidelines/thumbnail.webp)

Developing a C++ application dedicated to verifying MPI implementations against established performance guidelines. A tool designed to ensure optimal efficiency and compliance in high-performance computing environments.


---
Entwicklung einer C++-Anwendung zur √úberpr√ºfung von MPI-Implementierungen anhand festgelegter Leistungsrichtlinien. Ein Werkzeug, das optimale Effizienz und Konformit√§t in Hochleistungsrechenumgebungen sicherstellt.

## Key Facts

- Commercial Low Level Development Project
- 35 Workdays
- Client: Technical University of Vienna
- Language: English
- Volume: ‚Ç¨ 2000,--
- Team Size: 2

### Roles

- Software Engineer

## Project Goals

The primary objective of this thesis was the creation of 'pgchecker', a specialized C++ application designed to revolutionize the way MPI implementations are verified against performance guidelines. This tool was conceived to address the challenges in supercomputing, particularly in optimizing and assessing the efficiency of MPI operations. By automating this process, 'pgchecker' aims to provide a more thorough and accurate analysis, contributing to the improvement of supercomputing performance and the reliability of MPI implementations in diverse computational environments.


---
Das Hauptziel dieser Arbeit lag in der Entwicklung von 'pgchecker', einer speziell konzipierten C++-Anwendung, die einen neuen Weg in der √úberpr√ºfung von MPI-Implementierungen gem√§√ü Leistungsstandards ebnet. 'pgchecker' wurde geschaffen, um den spezifischen Anforderungen des Supercomputings gerecht zu werden, insbesondere im Hinblick auf die Optimierung und genaue Bewertung von MPI-Operationen. Durch die Automatisierung dieser Prozesse strebt 'pgchecker' an, eine tiefgehendere und exaktere Analyse zu erm√∂glichen, was sowohl die Leistung von Supercomputern als auch die Zuverl√§ssigkeit der MPI-Implementierungen in verschiedenen Einsatzbereichen merklich verbessert.

## Experience

In undertaking this project, I deepened my expertise in C++ programming and leveraged the capabilities of the PGMPITuneLib library for intricate performance analyses. The development and application of 'pgchecker' enabled me to explore the nuances of MPI operations in a supercomputing context, offering critical insights into their performance. This process involved rigorous testing and comparison of different MPI implementations, highlighting areas for optimization. My experience also extended to analyzing the impact of various collective operations on overall system efficiency, providing a comprehensive understanding of how these operations can be fine-tuned for superior performance.


---
In diesem Projekt konnte ich meine Kompetenzen in der C++-Programmierung wesentlich erweitern und die vielf√§ltigen M√∂glichkeiten der PGMPITuneLib-Bibliothek f√ºr komplexe Leistungsanalysen voll aussch√∂pfen. Mit der Entwicklung und dem Einsatz von 'pgchecker' gelang es mir, tiefe Einblicke in die Nuancen von MPI-Operationen im Kontext des Supercomputing zu gewinnen. Diese Erfahrung umfasste umfassende Tests und detaillierte Vergleiche unterschiedlicher MPI-Implementierungen, wobei ich wertvolle Erkenntnisse f√ºr m√∂gliche Optimierungen gewann. Dar√ºber hinaus erstreckten sich meine Erfahrungen auf die Analyse der Auswirkungen verschiedener kollektiver Operationen auf die Gesamteffizienz des Systems, was mir ein fundiertes Verst√§ndnis f√ºr die Feinabstimmung dieser Operationen zur Leistungssteigerung vermittelte.

## Skills

### Programming Languages

 - C++
### Technologies

 - MPI
 - Statistical Computing
 - High Performance Computing
 - CMake
 - UNIX
 - Git
 - Latex
### Soft Skill

 - Scientific Writing

## Visite

- [Book Automatic Identification of Violations Against MPI Performance Guidelines](https://files.hagn.network/documents/thesis/bachelor-software-engineering.pdf)
- [Book MPI is Good, Control is Better: Checking Performance Guidelines of Collectives](https://files.hagn.network/documents/thesis/mpi-publication.pdf)
- [Document Certificate Bachelor Software & Information Engineering](https://files.hagn.network/documents/certificate/bachelor-software-engineering.pdf)
- [Document Report Bachelor Software & Information Engineering](https://files.hagn.network/documents/certificate/bachelor-software-engineering-report.pdf)
- [Document Supplement Bachelor Software & Information Engineering](https://files.hagn.network/documents/certificate/bachelor-software-engineering-supplement.pdf)
- [GitHub Repository MPI Performance Guideline Checker](https://github.com/maxhagn/MPIPerformanceGuidelineChecker)
- [GitHub Repository PGMPITuneLib](https://github.com/hunsa/pgmpitunelib)
- [GitHub Repository ReproMPI Benchmark](https://github.com/hunsa/reprompi)

## Gallery

![Image 01 Book Preview Title Page](https://files.hagn.network/images/mpi-performance-guidelines/page-title.webp)
![Image 02 Book Preview Table of Content](https://files.hagn.network/images/mpi-performance-guidelines/page-toc.webp)
![Image 03 Book Preview Performance Guidelines](https://files.hagn.network/images/mpi-performance-guidelines/page-performance-guidelines.webp)
![Image 04 The collective operation MPI_Bcast.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-broadcast.webp)
![Image 05 The collective operation MPI_Allgatherv.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-allgatherv.webp)
![Image 06 One mockup-up algorithm for MPI_Bcast from a sequence consisting of the collective operations MPI_Scatter and MPI_Allgather.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-scatter_allgather.webp)
![Image 07 The basic point-to-point communication in MPI.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-point-to-point.webp)
![Image 08 The collective operation MPI_Scatter.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-scatter.webp)
![Image 09 The collective operation MPI_Gather.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-gather.webp)
![Image 10 The collective operation MPI_Reduce.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-collection-reduce.webp)
![Image 11 The collective operation MPI_Barrier.](https://files.hagn.network/images/mpi-performance-guidelines/mpi-barrier.webp)
![Image 12 The Functionality of the PGMPITuneLib library.](https://files.hagn.network/images/mpi-performance-guidelines/pgmpitunelib-diagram.webp)
![Image 13 The synopsis of pgchecker.](https://files.hagn.network/images/mpi-performance-guidelines/pgchecker-synopsis.webp)
![Image 14 The desired output of pgchecker.](https://files.hagn.network/images/mpi-performance-guidelines/pgchecker-output.webp)
![Image 15 The flow chart of pgchecker.](https://files.hagn.network/images/mpi-performance-guidelines/flow-chart-pgchecker.webp)
![Image 16 Class diagram representing the PGDataComparer superclass and the specific implementations AbsRuntimeComparer, RelRuntimeComparer, ViolationComparer, DetailedViolationComparer, and GroupedViolationComparer.](https://files.hagn.network/images/mpi-performance-guidelines/data-comparer-diagram.webp)
![Image 17 Class diagram representing the TwoSampleTest superclass and the specific implementations of TTest, WilcoxonMannWhitney, and WilcoxonRankSum.](https://files.hagn.network/images/mpi-performance-guidelines/two-sample-tests-diagram.webp)
![Image 18 Brave slowdown comparison for the OpenMPI, Version 4.1.4 implementation. Comparison of the runtime of the default algorithm to the mockup-ups. The higher the slowdown of the default algorithm, the more severe the violation found. If the slowdown of default compared to a mockup-up is in the red area, a violation against the guidelines occurs. In the green area no significant violation is detected. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1 core per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-openmpi-36x1.webp)
![Image 19 Brave slowdown comparison for the OpenMPI, Version 4.1.4 implementation. Comparison of the runtime of the default algorithm to the mockup-ups. The higher the slowdown of the default algorithm, the more severe the violation found. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 2 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-openmpi-36x2.webp)
![Image 20 Brave slowdown comparison for the OpenMPI, Version 4.1.4 implementation. Comparison of the runtime of the default algorithm to the mockup-ups. The higher the slowdown of the default algorithm, the more severe the violation found. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-openmpi-36x32.webp)
![Image 21 Brave slowdown comparison for the Open MPI implementation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes. The higher the slowdown, the more severe the violation found. Always the mockup-up with the shortest runtime is considered. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-comparison-openmpi.webp)
![Image 22 Brave slowdown comparison for the MVAPICH implementation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes. The higher the slowdown, the more severe the violation found. Always the mockup-up with the shortest runtime is considered. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-comparison-mvapich.webp)
![Image 23 Brave slowdown comparison for the MPICH implementation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes. The higher the slowdown, the more severe the violation found. Always the mockup-up with the shortest runtime is considered. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-comparison-mpich.webp)
![Image 24 Detailed analysis of the MPI_Bcast anomaly in MPICH. Performance comparison of the default implementation of the collective operation MPI_Bcast against the mockup-up algorithm consisting of the sequence of MPI_Scatter and MPI_Allgatherv. The experiment was performed on the Hydra system utilizing a total of 160 different numbers of processes.](https://files.hagn.network/images/mpi-performance-guidelines/bcast_anomaly_mpich.webp)
![Image 25 Brave slowdown comparison for the Intel ¬ÆMPI implementation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes. The higher the slowdown, the more severe the violation found. Always the mockup-up with the shortest runtime is considered. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-slowdown-comparison-intelmpi.webp)
![Image 26 Brave quality comparison for Intel ¬ÆMPI , MPICH , MVAPICH , and OpenMPI. Comparison of the violations detected across all collective operations by message size and number of cores used. For Brave, all violations found are counted. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-quality-comparison.webp)
![Image 27 Cautious quality comparison for Intel ¬ÆMPI, MPICH, MVAPICH, and OpenMPI. Comparison of the violations detected across all collective operations by message size and number of cores used. For Cautious, all violations subject to a MPI_Barrier warning are removed. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/cautious-quality-comparison.webp)
![Image 28 Violation comparison by compute nodes and cores per node. Comparison of the violations found against the performance guidelines of the Intel ¬ÆMPI, MPICH, MVAPICH, and OpenMPI implementations by number of compute nodes and cores per node. The experiment was performed on the Hydra system utilizing N = 36 compute nodes and n = 1, n = 2, and n = 32 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/violation-comparison.webp)
![Image 29 Slowdown comparison for Open MPI by collective operation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes and collective operations. Always the mockup-up with the shortest runtime is considered. The results are cautious and thus do not contain any violations which are subject to a MPI_Barrier warning. The experiment was performed on the Irene system utilizing N = 128 and N = 256 compute nodes and n = 1, n = 2, and n = 48 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/slowdown-comparison-openmpi.webp)
![Image 30 Slowdown comparison for Intel ¬ÆMPI by collective operation. Comparison of the slowdown of the default algorithm to the mockup-ups by message sizes and collective operations. Always the mockup-up with the shortest runtime is considered. The results are cautious and thus do not contain any violations which are subject to a MPI_Barrier warning. The experiment was performed on the Irene system utilizing N = 128 and N = 256 compute nodes and n = 1, n = 2, and n = 48 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/slowdown-comparison-intelmpi.webp)
![Image 31 Brave quality comparison for Intel ¬ÆMPI and Open MPI . Comparison of the violations detected across all collective operations by message size and number of cores used. The experiment was performed on the Irene system utilizing N = 128 and N = 256 compute nodes and n = 1, n = 2, and n = 48 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/brave-quality-comparison-intelmpi-openmpi.webp)
![Image 32 Cautious quality comparison for Intel ¬Æ MPI and Open MPI . Comparison of the violations detected across all collective operations by message size and number of cores used. The experiment was performed on the Irene system utilizing N = 128 and N = 256 compute nodes and n = 1, n = 2, and n = 48 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/cautious-quality-comparison-intelmpi-openmpi.webp)
![Image 33 Violation comparison by compute nodes and cores per node. Comparison of the violations found against the performance guidelines of the Intel ¬ÆMPI and Open MPI implementations by number of compute nodes and cores per node. The experiment was performed on the Irene system utilizing N = 128 and N = 256 compute nodes and n = 1, n = 2, and n = 48 cores per node.](https://files.hagn.network/images/mpi-performance-guidelines/violation-comparison-nodes-cores.webp)

